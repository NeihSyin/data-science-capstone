---
title: "Data Science Capstone Milestone Report - Exploratory Analysis"
author: "Timothy M. Rodriguez"
date: "Sunday, July 12, 2015"
output: html_document
---

## Summary
In this milestone we are going to first take an initial look at the data to get a better understanding of it and how to work with it. The data set is [HC Corpora](http://www.corpora.heliohost.org/). It is a set of text scraped from the internet in various languages.  We will be using the tm R package to due some exploratory analysis of the English data.

##Exploratory Analysis

First, we load the necessary packages and data.

```{r results='hide'}
require.or.install <- function(package.name) {
  if (!require(package.name, character.only=TRUE)) {
    install.packages(package.name, dep=TRUE)
    if (!require(package.name, character.only=TRUE)) {
      stop(paste("Could not install package ", package.name))
    }
  }
}

download.if.not.present <- function(url, file.name) {
  if (!file.exists(file.name)) {
    download.file(url, file.name)
  }
}

unzip.if.not.present <- function(zip.file.name, file.name) {
  if (!file.exists(file.name)) {
    unzip(zip.file.name, overwrite=TRUE, exdir=file.name)
  }
}

require.or.install("tm")
require.or.install("doParallel")
#require.or.install("Rgraphviz")
require.or.install("ggplot2")
require.or.install("slam")
registerDoParallel(makeCluster(detectCores()))

corpus.file.name <- 'corpus'
corpus.zip.name <- paste(corpus.file.name, ".zip")

download.if.not.present("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", corpus.zip.name)
unzip.if.not.present(corpus.zip.name, corpus.file.name)

```

## Sampling the data
The files are rather large at:
```{r}
twitter.file.name <- 'corpus/final/en_US/en_US.twitter.txt'
news.file.name <- 'corpus/final/en_US/en_US.news.txt'
blogs.file.name <- 'corpus/final/en_US/en_US.blogs.txt'

sapply(c(twitter.file.name, news.file.name, blogs.file.name), function(file.name) { paste(round(file.info(file.name)$size / 1024^2, digits=2), ' MB')})

```

We'd like to take a smaller sample of the data to work with inititally.  For exploratory purposes, let's start with 60/20/20 split for training/cross validation/test. It's important to split the data now so that we maintain a pristine test set that we have not considered even in our exploratory analysis.

```{r}
split.file <- function(file.name, splits) {
  con <- file(file.name)
  lines <- readLines(con)
  print(paste("Found", length(lines), "lines. In file", file.name))
  print("Splitting the file into a training, testing, and cross validation set")
  line.splits <- runif(length(lines))
  sub.file.name <- substr(file.name, 1, nchar(file.name) - 4)
  train.file <- file(paste(sub.file.name, ".train.txt", sep=""), open="at")
  test.file <- file(paste(sub.file.name, ".test.txt", sep=""), open="at")
  cross.file <- file(paste(sub.file.name, ".cross.txt", sep=""), open="at")
  
  num.train <- 0
  num.cross <- 0
  num.test <- 0
  
  for (line.index in 1:length(lines)) {
    split <- line.splits[line.index]
    if (split <= splits[1]) {
      writeLines(lines[line.index], train.file)
      num.train <- num.train + 1
    } else if (split <= splits[1] + splits[2]) {
      writeLines(lines[line.index], cross.file)
      num.cross <- num.cross + 1
    } else {
      writeLines(lines[line.index], test.file)
      num.test <- num.test + 1
    }
  } 
  close(train.file)
  close(cross.file)
  close(test.file)
  close(con)
  print(paste(num.train, "lines in training set."))
  print(paste(num.cross, "lines in cross validation set."))
  print(paste(num.test, "lines in test set."))
  
}
```

```{r}
splits <- c(0.6, 0.2, 0.2)
split.file(twitter.file.name, splits)
split.file(news.file.name, splits)
split.file(blogs.file.name, splits)
```

## Frequency Analysis

Here we do an frequency analysis on sample of the training set. To reduce the dimensionality of the data we apply a series of transforms to strip non-essential characters, lower case words, stem the text, and remove common words (stop words) as well as profanity.

```{r}
set.seed(1234)
sample.percent <- 0.001 #limited memory on my machine
twitter.train <- readLines("corpus/final/en_US/en_US.twitter.train.txt")
news.train <- readLines("corpus/final/en_US/en_US.news.train.txt")
blogs.train <- readLines("corpus/final/en_US/en_US.blogs.train.txt")

create.sampled.corpus <- function(training.set, sample.percent) {
  print(paste("Sampling corpus at", sample.percent))
  Corpus(VectorSource(sample(training.set, sample.percent*length(training.set))))
}

corpi <- sapply(list(twitter.train, news.train, blogs.train), create.sampled.corpus, sample.percent)

combined.corpus <- c(corpi[[1]], corpi[[2]], corpi[[3]])

profanity <- readLines("profanity/en_US/profanity.txt")

clean.corpus <- function(corpus) {
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("english"), profanity))
  corpus <- tm_map(corpus, stemDocument, language = "english")
  corpus
}

cleaned.corpus <- clean.corpus(combined.corpus)
rm(corpi, combined.corpus, twitter.train, news.train, blogs.train)
```
After sampling and cleaning the data, we have a corpus of `r length(cleaned.corpus)`

Now that we have a cleaned corpus, let's do some analysis on frequencies.

```{r}
dtm <- DocumentTermMatrix(cleaned.corpus)
```
After converting the corpus to a document term matrix we still have `r nrow(dtm)` with `r ncol(dtm)` distinct terms/words. After all the data cleaning, it is important to note that the total is reduced by dimension reducing exercises such as lower casing, punctuation stripping, stemming, stopword removal, etc. The total number of words across the sampled corpus is `r sum(col_sums(dtm))`

```{r}
freq <- sort(col_sums(dtm), decreasing=TRUE)
rm(dtm)
word.freq <- data.frame(word=names(freq), freq=freq)
rm(freq)

#dtm <- removeSparseTerms(doc.term.matrix, 0.4)
ggplot(aes(reorder(word, -freq), freq), data=subset(word.freq, freq > 300)) + geom_bar(stat="identity") + theme(axis.text.x=element_text(angle=45, hjust=1))

```

One can see here that the distribution follows a power law and the next word diminishes in frequency quite strongly after the last.  This distribution would be much more clear if more data had been indexed.  However, only a small sample was able to be taken due to compute resource constraints.

## Summary

Now that there is a text processing pipeline for ingesting and further analyzing the text.  This combined sampled corpus can be used for creating a mixed ngram model to predict the next word after a set of words (as is used for predictive text systems such as SwiftKey).